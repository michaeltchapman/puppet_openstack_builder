# This is the sample user.yaml for the stacktira scenario
# For additional things that can be configured, look at
# user.stacktira.yaml, or user.common.
#
# Warning:
# When working with non-string types, remember to keep yaml
# anchors within a single file - hiera cannot look them
# up across files. For this reason, editing the lower section
# of this file is not recommended.

scenario: stacktira

networking: none
storage:    none
monitoring: none
# The default network config is as follows:
# eth0: vagrant network in testing
# eth1: deploy network
# eth2: public api network
# eth3: private service network + GRE
# eth4: external data network

build_server_name: build-server
build_server_ip: 192.168.242.100

# These are legacy mappings, and should have no effect
controller_public_address: 10.2.3.5
controller_internal_address: 10.3.3.5
controller_admin_address: 10.3.3.5

# Interface that will be stolen by the l3 router on
# the control node.
external_interface: eth4
# for a provider network on this interface instead of
# an l3 agent use these options
#openstacklib::openstack::provider::interface: eth4
#neutron::plugins::ovs::network_vlan_ranges: default

# Gre tunnel address for each node
internal_ip: "%{ipaddress_eth3}"

# This is the interface that each node will be binding
# various services on.
deploy_bind_ip: "%{ipaddress_eth1}"
public_bind_ip: "%{ipaddress_eth2}"
private_bind_ip: "%{ipaddress_eth3}"

# The public VIP, where all API services are exposed to users.
public_vip: 10.2.3.5

# The private VIP, where internal services are exposed to openstack services.
private_vip: 10.3.3.5

# List of IP addresses for controllers on the public network
control_servers_public: &control_servers_public  [ '10.2.3.10', '10.2.3.11', '10.2.3.12']

# List of IP addresses for controllers on the private network
control_servers_private: &control_servers_private  [ '10.3.3.10', '10.3.3.11', '10.3.3.12']

# A hash of hostnames to private network IPs. Used for rabbitmq hosts
# resolution
openstacklib::hosts::cluster_hash:
  control1private:
    ip: '10.3.3.10'
  control2private:
    ip: '10.3.3.11'
  control3private:
    ip: '10.3.3.12'

# List of controller hostnames. Used for rabbitmq hosts list
cluster_names: &cluster_names  [ 'control1private', 'control2private', 'control3private' ]

# For the case where the node hostname already resolves to something else,
# force the nodename to be the private shortname we're using above.
rabbitmq::environment_variables:
  'NODENAME': "rabbit@%{hostname}private"

#Libvirt type
nova::compute::libvirt::libvirt_virt_type: qemu

horizon::wsgi::apache::bind_address: "%{ipaddress_eth2}"

# Use these to set an apt proxy if running on a Debian-like
apt::proxy_host: 192.168.0.18
apt::proxy_port: 8000

# CIDRs for the three networks.
deploy_control_firewall_source: '192.168.242.0/24'
public_control_firewall_source: '10.2.3.0/24'
private_control_firewall_source: '10.3.3.0/24'

# Proxy configuration of either apt or yum
openstacklib::repo::apt_proxy_host: '192.168.0.18'
openstacklib::repo::apt_proxy_port: '8000'
openstacklib::repo::yum_http_proxy: 'http://192.168.0.18:8000'
openstacklib::repo::yum_epel_mirror: 'http://fedora.mirror.uber.com.au/epel'
openstacklib::repo::yum_base_mirror: 'http://centos.mirror.uber.com.au'

enabled_services: &enabled_services
  - keystone
  - glance
  - nova
  - neutron
  - cinder

# Openstack version to install
openstack_release: icehouse
openstacklib::repo::uca::release: icehouse
openstacklib::repo::rdo::release: icehouse
openstacklib::compat::release: icehouse

#########################################
# Anchor mappings for non-string elements
#########################################

neutron::rabbit_hosts: *cluster_names
nova::rabbit_hosts: *cluster_names
cinder::rabbit_hosts: *cluster_names
rabbitmq::cluster_nodes: *cluster_names
openstacklib::loadbalance::haproxy::cluster_names: *cluster_names
openstacklib::loadbalance::haproxy::ceilometer::cluster_names: *cluster_names
openstacklib::loadbalance::haproxy::cinder::cluster_names: *cluster_names
openstacklib::loadbalance::haproxy::dashboard::cluster_names: *cluster_names
openstacklib::loadbalance::haproxy::glance::cluster_names: *cluster_names
openstacklib::loadbalance::haproxy::heat::cluster_names: *cluster_names
openstacklib::loadbalance::haproxy::keystone::cluster_names: *cluster_names
openstacklib::loadbalance::haproxy::mysql::cluster_names: *cluster_names
openstacklib::loadbalance::haproxy::neutron::cluster_names: *cluster_names
openstacklib::loadbalance::haproxy::nova::cluster_names: *cluster_names
openstacklib::loadbalance::haproxy::rabbitmq::cluster_names: *cluster_names

openstacklib::loadbalance::haproxy::ceilometer::cluster_addresses: *control_servers_public
openstacklib::loadbalance::haproxy::cinder::cluster_addresses: *control_servers_public
openstacklib::loadbalance::haproxy::dashboard::cluster_addresses: *control_servers_public
openstacklib::loadbalance::haproxy::glance::cluster_addresses: *control_servers_public
openstacklib::loadbalance::haproxy::heat::cluster_addresses: *control_servers_public
openstacklib::loadbalance::haproxy::keystone::cluster_addresses: *control_servers_public
openstacklib::loadbalance::haproxy::neutron::cluster_addresses: *control_servers_public
openstacklib::loadbalance::haproxy::nova::cluster_addresses: *control_servers_public

openstacklib::loadbalance::haproxy::mysql::cluster_addresses: *control_servers_private
openstacklib::loadbalance::haproxy::rabbitmq::cluster_addresses: *control_servers_private
openstacklib::loadbalance::haproxy::keystone::cluster_addresses: *control_servers_private
galera::galera_servers: *control_servers_private

openstacklib::openstack::databases::enabled_services: *enabled_services
openstacklib::openstack::endpoints::enabled_services: *enabled_services
